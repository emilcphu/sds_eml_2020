{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10: Networks 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "Networks are mathematical representations of complex systems. We can use networks to gain various statistical insight about the system we're representing, and we can look for patterns at the meso-scale by employing *community detection* algorithms. This week we will explore the following:\n",
    "\n",
    "* Network null models\n",
    "* How to use a null model to infer the p-value of a result\n",
    "* How a popular community detection algorithm works (and fails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**: Outside of class, use [issue on GitHub](https://github.com/abjer/tsds/issues) for asking questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T10:56:27.700322Z",
     "start_time": "2019-03-12T10:56:27.695950Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import networkx as nx           # `pip install networkx`\n",
    "import json\n",
    "from collections import Counter\n",
    "import community                # `pip install python-louvain` or `conda install -c auto python-louvain`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Network null models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a null model?** Null models are alternative instances of data, that are used to assess the amount of signal that is due to pure randomness. For example, you might have\n",
    "measured some signal in your data, like a large number of triangles (high average local clustering coefficient)\n",
    "in your network, but before you go and report that to your boss you need to answer one crucial question: how does\n",
    "this result compare with a *random* one? In other words, how can you be so sure that this high number of triangles\n",
    "measured is not perfectly normal even in a random network of similar origin and therefore not very special after all? The answer: **you\n",
    "create a null model to compare your result with!**\n",
    "\n",
    "> Note: null models are a concept from the general field of statistics and therefore not just specific to\n",
    "networks. You can use this statistical tool anytime you need to assess how likely it is that your result is random.\n",
    "\n",
    "The most common type of null model is one where you shuffle links in your network, while preserving the degree\n",
    "sequence. *(Recall, that the degree sequence is a list that stores the degree of each node. So if we shuffle and\n",
    "preserve the degree sequence at the same time, it means that after all the link-shuffling is done, nodes will\n",
    "have new neighbors, but the same number of neighbors)*. In this shuffled network (the null model), if you find that\n",
    "there are far fewer triangles than in your real data then you can start to argue that your result is significant.\n",
    "\n",
    "> Note: the term *null model* is a slight misnomer, as it is not a model per se, but rather an instance of the\n",
    "data that is permuted in some way (usually under constraints, such as preservation of degree sequence), so it can\n",
    "be taken to represent *randomness*. As such, there is no *model*, but rather an *instance* or simply *data*.\n",
    "\n",
    "But that is just one comparison. What if the number of triangles in the random data–the null model–is smaller, but\n",
    "not that much smaller? Can you still say your result is significant? Well, the trick (although computationally expensive\n",
    "as you will come to learn) is to do MANY comparisons. 1000 is not a bad start. For each comparison, you check if\n",
    "the number of triangles in your real data is bigger. After you have compared one thousand times you compute **the\n",
    "fraction of times** your number of triangles in the real data was bigger than in the random data. Guess what that\n",
    "number (between 0 and 1) is called? **The p-value**. And what does it communicate? **The probability that your result\n",
    "is random!**\n",
    "\n",
    "^ That's some useful statistics right there!\n",
    "\n",
    "Please make sure you have **READ AND UNDERSTOOD** the above, as you will otherwise have a very hard time completing the\n",
    "following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 10.1.1**: The method described above works for anything you might want to measure in a network. Let's say,\n",
    "instead of measuring the number of triangles, you measured the network diameter. Explain in your own words how to\n",
    "assess the statistical significance (the p-value) of such a measurement, using the same null model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 10.1.2**: The null model described above rewires a network while preserving the degree distribution. [Here](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.swap.double_edge_swap.html#networkx.algorithms.swap.double_edge_swap) is\n",
    "implementation of it in NetworkX. In your own words, describe:\n",
    "1. how it works and why it achieves randomness\n",
    "without changing the degree distribution.\n",
    "2. Also describe what can sometimes happen and why the desired number of swaps `nswaps` is only an upper-bound on\n",
    "the number of swaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ex. 10.1.3**: Load the Facebook wall-post network from last week, into a `networkx.Graph` object called `G`.\n",
    "1. Measure the average local clustering coefficient (ALCC) of `G`. Print it.\n",
    "2. Over 1000 iterations, measure the ALCC for `G` where, in each iteration, you have made 1000 edge swaps using \n",
    "`double_edge_swap`. Append the measured ALCC values to a list. Print the average of this list. *Note: this will\n",
    "obviously take some time**.\n",
    "3. Report the p-value of your result, the ALCC of the real data.\n",
    "4. Make a histogram that displays the distribution of ALCC values in the null models as a histogram as well as\n",
    "the ALCC of the real data as a vertical line. Comment on this result. Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Communities are little lumps of nodes in a network that are unusually strongly connected. Your family is a community, and your friend group from work or university is another community. While there is no one true definition of what a community is, there are many many different methods and algorithms for finding them. Here we will work with one of the most popular ones: [Louvain Modularity](https://en.wikipedia.org/wiki/Louvain_Modularity). The following exercises will walk you through the fundamentals of this method, and finally have you apply it to the network you used last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The modularity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.1**: Assume you have a network where nodes and links tend to form lumps here and there. Imagine you now reach for your pen, and start labeling these nodes with group names (or ids) that feel appropriate according to how they are lumped together. If your partition is \"good\", nodes that are connected in groups should intuitively have the same label, while nodes that are distant and disconnected should have different labels. Modularity is a function that can be used to measure, by this logic, *how good* your partition is. It is in technical terms a *utility function*, and it looks like this:\n",
    "> <br><br>\n",
    "> $$ Q = \\dfrac{1}{2m}\\sum_{ij}{\\left[A_{ij}-\\dfrac{k_ik_j}{2m}\\right]\\delta(c_i, c_j)}.$$\n",
    "> <br>\n",
    "> Your job in this problem is to explain this equation. When I look at daunting math I find it calming to try and read it as if it were code. Since all math can be implemented in code, all math can be broken into parts, where each part does a seperate thing. Answer each question below seperately:\n",
    "1. In code, a sum, $\\sum$, is like a `for` loop, where in every iteration you increment a variable. In the equation for modularity the little $ij$ subscript tells is what the sum is looping over (like `for ij in sumloop`). But what is $ij$?\n",
    "2. In each iteration of the sum, the delta function $\\delta(c_i, c_j)$ is used, where $c_i$ is the community label of node $i$. The delta function is a very simple program that returns 0 if the two input values are different and 1 if they are they same. How would you implement the delta function in code? What is it used for in the modularity equation?\n",
    "3. Inside the sum we use the term $\\frac{k_ik_j}{2m}$ as our *null model*. $k$ is the degree sequence (so $k_i$ is the degree of node $i$) and $m$ is the sum of all link weights. Explain what this null model measures. Could we have used other null models?\n",
    "4. The sum subtracts the null model from $A_{ij}$ and adds the result to its final value if the delta function evaluates to 1. What is the point of only summing over this difference when the delta function is 1?\n",
    "5. The sum term is normalized by $2m$. Why exactly $2m$?\n",
    "6. Summarize your insight gained from answering the above questions. In your own words, explain how the modularity function works. Use 1-3 sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.2**: Implement the modularity function. Write a Python function that takes as input an adjacency matrix and a label vector, and returns the modularity. Compute and print the modularity for the ones given below. The correct result is 0.122."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T10:27:43.909671Z",
     "start_time": "2019-03-12T10:27:43.900257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [0, 1, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 1],\n",
    "    [0, 0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 1, 0],\n",
    "])\n",
    "\n",
    "c = [0, 0, 0, 0, 1, 1]\n",
    "\n",
    "def modularity(A, c):\n",
    "    \"\"\"Compute modularity for a labeled network.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "        A : numpy.array\n",
    "            Adjacency matrix. (N, N) square matrix.\n",
    "        c : list of ints\n",
    "            Community labels. Length N.\n",
    "    \n",
    "    Output\n",
    "    ------\n",
    "        out : float\n",
    "    \"\"\"\n",
    "    # Your beautiful code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.3**: The example labeling, `c`, was not optimal. Find the optimal one and print its modularity score.\n",
    "\n",
    ">*Hint: Either just try a bunch of different label combinations or visualize the network so you can see what is optimal. Using pen and paper here is no shame.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the best communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we are now able to evaluate the quality of a partition. But how do we find the best partition? Modularity gives us a way to measure *how good* our partition is, but it does not tell of how to find the best one. For that we need some sort of algorithm. The *Louvain method* is that algorithm.\n",
    "\n",
    "It works in the following steps:\n",
    "1. Set every node to be its own community (initiate `c = [0, 1, 2, 3, 4, 5]`).\n",
    "2. Compute the modularity.\n",
    "3. Now pick a random node.\n",
    "    1. For every neighbor it has, try giving it the neighbor's label, and compute the change in modularity.\n",
    "    2. If any of those relabelings led to an increase in modularity, choose the relabeling with the greatest increase.\n",
    "4. Repeat 2-3 until modularity ceases to increase for any relabelings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.BONUS**: Implement the Louvain method, and show that it gives the labeling for `A`. A cool portfolio project for your Github account here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Communication communities on Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with some real data. Whip out the network you created **last week**, we will be using that again. Apply again the **threshold** you created in **Ex. 9.2.4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.4**: Find the communities in this networks. Print the number of communities and plot the distribution of community sizes. See something interesting? Comment on this distribution.\n",
    "\n",
    ">*Hint: You're welcome to use your own implementation of the Louvain algorithm (pretty badass if you can), but there's also a widely used Python implementation that you can take off the shelf. Go ahead and install `python-louvain` by running `conda install -c auto python-louvain` in a terminal. After installation, import it with `import community`, and use `community.best_partition` to get a node-community dictionary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.5**: Visualize the network, similarly to how you did it last week but this time coloring the nodes by their labels.\n",
    ">\n",
    "> *Hint: [Here](https://netwulf.readthedocs.io/) are the `netwulf` docs. Fiddle around with the layout a little it always makes the network look nicer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 10.2.6:** Maybe the communities you observed in the previous exercise were not as pretty as you were hoping.\n",
    "Admittedly, the Facebook wallpost network is not the most modular network anyway, but still it examplifies a serious\n",
    "problem with the Modularity score as a utility function for community detection. Can you explain what this problem is,\n",
    "and why it becomes increasingly severe as the network grows larger?\n",
    ">\n",
    "> *Hint: it has something to do with the null model that Modularity uses*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Final note: there are many other community detection algorithms out there. Check out the [NetworkX docs](https://networkx.github.io/documentation/stable/reference/algorithms/community.html)\n",
    "for some easy-to-use alternatives to Modularity. Also you may want to have a look at [Infomap](https://mapequation.github.io/infomap/python/)\n",
    "especially if you are working with networks where links represent flow (like transactions, exchange, citations, hyperlinks, etc). It usually gives amazing results.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
